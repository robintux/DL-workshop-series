{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_loss_functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/DL-workshop-series/blob/master/Part%20II%20-%20Learning%20in%20Deep%20Networks/custom_loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mjJLHMpByt9w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Custom loss functions\n",
        "\n",
        "- mae, mse (predifined)\n",
        "- mce (mean cubed error)\n",
        "- mpe (mean power error)\n",
        "- double_loss\n",
        "- weighted_double_loss"
      ]
    },
    {
      "metadata": {
        "id": "_ODgwFquPIPA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook we will see how we can define custom loss functions for a kreas model training\n",
        "First of all we run some necessary imports"
      ]
    },
    {
      "metadata": {
        "id": "MtqkH7mdPYyH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "from keras.models import Sequential\n",
        "from keras.losses import mae, mse\n",
        "import keras.backend as K\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eVDGtmbPZJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's build a very simple model that returns the input unchanged. The simpler way to do it is using a Lambda layer and applying the lambda function $f(x) = x$"
      ]
    },
    {
      "metadata": {
        "id": "OpiU397IPn_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential([Lambda(lambda x: x, input_shape=(1,))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuP4cBTrPsfh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can test our model by feeding an array of three numbers and check that the result is the same array"
      ]
    },
    {
      "metadata": {
        "id": "Jtbio0i1O5OL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [0, 1, 2]\n",
        "x = np.array(x)\n",
        "y_pred = model.predict_on_batch(x)\n",
        "print(*y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "grb28WJsP97M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try to see how a loss function would work on this model. First of all we have to compile our model with a specific optimizer and loss function. The choice of the optimizer is not important since we are not going to train the model.\n",
        "\n",
        "For loss function let's define the Mean Absolute Error (mae). This loss function takes as input the ground truth values ($y\\_true$) and the model's predictions ($y\\_pred$) tensors and returns the mean absolute difference of them:\n",
        "$$ mae = \\frac{\\sum_{i=0}^N{|y\\_true_i - y\\_pred_i|}}{N}$$"
      ]
    },
    {
      "metadata": {
        "id": "2ELYav_bP6MG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile('sgd', mae)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wdK5k-9yQzgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to get the loss the model needs the $y\\_true$ and $y\\_pred$ tensors as mentioned before.\n",
        "- The $y\\_pred$ tensor will be the same as the input array x.\n",
        "\n",
        "- The $y\\_true$ tensor has to be defined by us. For simplicity we will define:\n",
        "$$x = 0$$\n",
        "$$y = 4$$\n",
        "\n",
        "In this case we have\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the mean absolute difference is $|0 - 4| = 4$"
      ]
    },
    {
      "metadata": {
        "id": "T0ij4_GPQ21s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwO2OGPgR5O6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's run the evaluation of the model keeping the same arrays but changing the loss function. In this case we will use the Mean Squared Error (mse):\n",
        "\n",
        "This loss function takes as input the ground truth values ($y\\_true$) and the model's predictions ($y\\_pred$) tensors and returns the mean squared difference of them:\n",
        "$$ mse = \\frac{\\sum_{i=0}^N{(y\\_true_i - y\\_pred_i)^2}}{N}$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I5zSUm9ZR2Lc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile('sgd', mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XEmh-B2wR2Le"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the mean squared difference is $(0 - 4)^2 = 16$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2_GcZ4cPR2Le",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y3FV8AweTR90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try to define our own loss function. The definition of a loss function for a keras model looks like any python function definition. However, for we are only allowed to use the basic python mathematic operators (+-*/ etc.) and the keras.backend operators (K.abs, K.pow etc.)\n",
        "\n",
        "We will try to define the Mean Cubed Error (mce):\n",
        "$$ loss = \\frac{\\sum_{i=0}^N{|y\\_true_i - y\\_pred_i|^3}}{N}$$"
      ]
    },
    {
      "metadata": {
        "id": "q1OkgdaLULQJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mce(y_true, y_pred):\n",
        "  return K.mean(K.pow(K.abs(y_true - y_pred), 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pwN5O6EDU2Rq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile('sgd', mce)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdI3-iczUKXH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the mean cubed difference is $|0 - 4|^3 = 64$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A8Mcn27IU8Lh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Akd2xowIVDQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is how one can easily define a new loss function to train a keras model.\n",
        "Actually it can be even more general.\n",
        "\n",
        "One can define a loss function for the mean error to any power:"
      ]
    },
    {
      "metadata": {
        "id": "n9s8KTr2VSdO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mpe(y_true, y_pred, p=4):\n",
        "  return K.mean(K.pow(K.abs(y_true - y_pred), p))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bEY_RH_sVh43",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile('sgd', mpe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2hqgWYaJVX2q"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the mean difference to the power of 4 is $|0 - 4|^4 = 256$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VKQUcvLgVX2r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sEquKw_fVtWl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However using mpe() as it is means that one has to define the power at the function definition. This is not very convenient since it could be the case that the loss function definition is at a different file than the model compilation. In this case we can use a very useful python tool: the *partial()*\n",
        "partial() is a function that takes as first argument another function and one or more arguments of this function and returns a function that is the same with the original one but with the denoted arguments as default values. For more details please check [here](https://docs.python.org/2/library/functools.html#functools.partial)"
      ]
    },
    {
      "metadata": {
        "id": "oQRnl32BW2CE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mpe(y_true, y_pred, p):\n",
        "  return K.mean(K.pow(K.abs(y_true - y_pred), p))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBupljGaW3Cv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m5e = partial(mpe, p=5)\n",
        "model.compile('sgd', m5e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ofO66jwEXC0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the mean difference to the power of 5 is $|0 - 4|^5 = 1024$\n",
        "\n",
        "Notice that the mpe() definition does not have any default value for *p*. This means that it can be at a different file and we can import it like this for example:\n",
        "\n",
        "`from my_losses import mpe`"
      ]
    },
    {
      "metadata": {
        "id": "hoaAdGihaxpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QJ8fXNIuaQs8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another thing that we can do is define a loss function as a combination of two loss functions. In this case the result can be the average loss from the two loss functions"
      ]
    },
    {
      "metadata": {
        "id": "ouxHBENnadNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def double_loss(y_true, y_pred, l1, l2):\n",
        "  loss_1 = l1(y_true, y_pred)\n",
        "  loss_2 = l2(y_true, y_pred)\n",
        "  \n",
        "  return (loss_1 + loss_2) / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "04k-Rqh6a1OJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m5e = partial(mpe, p=5)\n",
        "mae_m5e = partial(double_loss, l1=mae, l2=m5e)\n",
        "model.compile('sgd', mae_m5e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cEQ3dPZHa1OK"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the average of mae and m5e is\n",
        "$$\\frac{|0 - 4| + |0 - 4|^5 }{2}=514$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zZ8cyjzAa1OL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DnaZrQXWcN0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Actually we can go even further and define the weight average of the two loss functions.\n",
        "\n",
        "Let's say for example that we want the second loss to have different weight in the final results than the first one. In this case we just have to multiply the losses by the (normalized) weights.\n",
        "\n",
        "We can define the default values to be equal (e.g. 1) and then change only the weight of the loss that we want.\n",
        "\n",
        "In our example we will assign to the m5e loss twice the weights of that of mae loss."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "encDxkKTcnSG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weighted_double_loss(y_true, y_pred, l1, l2, w1=1, w2=1):\n",
        "  w1, w2 = w1 / (w1+w2), w2 / (w1+w2)\n",
        "  loss_1 = l1(y_true, y_pred) * w1\n",
        "  loss_2 = l2(y_true, y_pred) * w2\n",
        "  \n",
        "  return loss_1 + loss_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U522VzDUcnSH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m5e = partial(mpe, p=5)\n",
        "mae_m5e = partial(weighted_double_loss, l1=mae, l2=m5e, w2=2)\n",
        "model.compile('sgd', mae_m5e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yGLTjP8scnSJ"
      },
      "cell_type": "markdown",
      "source": [
        "We keep the same values for $y\\_true$ and $y\\_pred$:\n",
        "$$y\\_pred = 0$$\n",
        "$$y\\_true = 4$$\n",
        "\n",
        "which means that the weighted average of mae (x1) and m5e (x2) is\n",
        "$$|0 - 4|\\frac{1}{3} + |0 - 4|^5\\frac{2}{3}=684$$"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zqPHOopwcnSJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = [[0]]\n",
        "x = np.array(x)\n",
        "\n",
        "y = [[4]]\n",
        "y = np.array(y)\n",
        "\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wPQeKaqyZzW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can define your own configurable loss function to train your model.\n",
        "\n",
        "In a similar way you can also define a metric function to evaluate its performance\n",
        "\n",
        "## The end"
      ]
    }
  ]
}