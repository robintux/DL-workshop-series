{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/DL-workshop-series/blob/master/Part%20II%20-%20Learning%20in%20Deep%20Networks/regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "chyfoNdmPE4O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Input, BatchNormalization\n",
        "from keras.initializers import constant\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bVHAUL4uR36C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization\n",
        "\n",
        "In this very simple example we have a network with:\n",
        "- one input of shape (1,) (or simply stated the input is one number)\n",
        "- one Dense layer with only one unit (aka neuron)\n",
        "\n",
        "When we print the summary of the model we can see that there is a total of 2 parameters, both of which are trainable.\n",
        "\n",
        "The first of these parameters is the weights (*w*) of the unit and the second is the bias (*b*)\n",
        "the function inside the unit is:\n",
        "$$f(x)=wx+b$$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now if we uncomment the commented line and re-run the model we will see that there is one more layer in the end of the model summary, the BatchNormalization layer.\n",
        "\n",
        "This time there is a total of 6 parameters:\n",
        "\n",
        "- the initial 2 trainable parameters of the Dense layer\n",
        "- 2 trainable parameters of the batch_normalization layer\n",
        "- 2 non-trainable parameters of the batch_normalization layer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "These parameters come from the above function of the batch normalization layer:\n",
        "$$\\hat h = \\gamma \\frac{h-\\mu_B}{\\sigma_B}+\\beta$$\n",
        "\n",
        "where:\n",
        "- $h$ and $\\hat h$ are the hidden values before and after the normalization\n",
        "- $\\mu_B$ and $\\sigma_B$ represent the mean and the standard deviation of $h$.\n",
        "\n",
        "  The are estimated within a batch of M samples. These are the **non trainable** parameters (since they are computed from the batch)\n",
        "-  $\\gamma$ is a scale parameter and $\\beta$ is a shift parameter.\n",
        "\n",
        "  These are the **trainable** paramters. We can define if we want the layer to make use of them or not. By changing the values of the *center* and *scale* arguments to *False* the layer does not make use of these parameters and thus we do not have these 2 trainable parameters\n"
      ]
    },
    {
      "metadata": {
        "id": "biTxyH_jPFg_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = Input([1])\n",
        "output = Dense(1, kernel_initializer=constant(2), bias_initializer=constant(1))(input)\n",
        "# output = BatchNormalization(center=False, scale=False)(output)\n",
        "model = Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SfbF2MLSwbN5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's see how it works on a specific example. In order to simplify the model even more we will define a model with an input layer of 1 number and a Batch Normalization layer on top of it.\n",
        "\n",
        "This means that the input numbers will pass directly through the batch_norm layer and we will get its output.\n",
        "\n",
        "We define center and scale to be False so there are no trainable parameters.\n",
        "\n",
        "We also define the momentum and the epsilon to be 0 in order to get the results based on the formula presented above (otherwise the results will be different). It is recomended when using this layer in real applications **not** to set these parameters to 0."
      ]
    },
    {
      "metadata": {
        "id": "_9Q09E3AZt7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = Input([1])\n",
        "output = BatchNormalization(center=False, scale=False, momentum=0, epsilon=0)(input)\n",
        "model = Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZ06aF7yxaOy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In our example we will use as input an array with 2 elements: 1 and 2.\n",
        "\n",
        "We reshape the array so that the model accepts the two numbers as a batch of two elements.\n",
        "\n",
        "When we get the output of the model however we see that the numbers remained unchainged..."
      ]
    },
    {
      "metadata": {
        "id": "GHSw5dTVWuRC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = 1, 2\n",
        "x = np.reshape(np.array(x), (2, 1))\n",
        "\n",
        "y_pred = model.predict(x)\n",
        "print(*y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtYOgKiVxyVv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now this happened because the (non trainable) weights of the model ($\\mu_B$ and $\\sigma_B$) were not calculated. The values of these parameters are the initial ones (0 and 1)"
      ]
    },
    {
      "metadata": {
        "id": "M_jLBZHbgEd0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(*model.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OdNVbqdVyl7o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All the parameters of the model, even the non trainable, are calculated during the training phase of the model and are retained during the inference phase.\n",
        "\n",
        "Thus we have to \"train\" our model on our batch.\n",
        "\n",
        "In order to train the model we first have to compile it with a specific otpimizer and loss function. The choice of these two arguments is arbitrary, as it is the choice of the y values. Thus we can safely use 'sgd', 'mae' and 'x' without loss of generality."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5nLc-D3-hsMF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile('sgd', 'mae')\n",
        "t = model.train_on_batch(x, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDLVj8TozfjJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now if we run again the previous cell and print the model's weights we will get the updated mean and standard deviation (actually the variance) based on the batch"
      ]
    },
    {
      "metadata": {
        "id": "Tq1zvAvFjdKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "mean:\n",
        "$$\\bar x=\\frac{\\sum^N_{i=1}x_i}{N}$$\n",
        "standard deviation:\n",
        "$$\\sigma=\\sqrt{\\frac{\\sum^N_{i=1}(x_i-\\bar x)^2}{N-1}}$$\n",
        "variance:\n",
        "$$Var=\\sigma^2$$"
      ]
    },
    {
      "metadata": {
        "id": "JrLt2y9Uz7OT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can rerun the prediction cell and obtain the new outputs of the model. The two outputs have indeed mean = 0 and std = 1.\n",
        "\n",
        "If you want to check it you can set the h variable at the next model to be equal to y_pred"
      ]
    },
    {
      "metadata": {
        "id": "tkCUYIyMigWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "h = np.array([1, 2])  # y_pred\n",
        "mean = np.sum(h) / len(h)\n",
        "std = np.sqrt(np.sum(np.square(h - mean)) / (len(h) - 1))\n",
        "var = std**2\n",
        "print('mean: %.2f\\nstd: %.3f\\nvar: %.2f' % (mean, std, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37fcO_ZCV7Zr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## L1 and L2 regularization"
      ]
    },
    {
      "metadata": {
        "id": "VsIIt-lH0umx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this example we define a simple model with an input layer of one number and one Dense layer with one unit (aka neuron).\n",
        "\n",
        "However, for the specific unit we set the values of the weight and the bias during the initialization:\n",
        "\n",
        "$$f(x)=wx+b$$\n",
        "\n",
        "where $w=2$ and $b=1$\n",
        "\n",
        "We also explicitly define the kernel, bias and activity regularizers to be None (which is their default value)\n"
      ]
    },
    {
      "metadata": {
        "id": "GlIHA4_bPHT0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = Input([1])\n",
        "output = Dense(1, kernel_initializer=constant(2), bias_initializer=constant(1),\n",
        "               kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None)(input)\n",
        "model = Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y4RLPjxH1_-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "now if we run the model on a specific number, let's say 2, we can see that we get the correct result (5)"
      ]
    },
    {
      "metadata": {
        "id": "-F5C2XTmP41e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = 2\n",
        "y_pred = model.predict(np.array((x,)))\n",
        "y_pred[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCEhuZ4B125K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So when we compile the model and evaluate it with the correct numbers we see that the loss is equal to 0"
      ]
    },
    {
      "metadata": {
        "id": "ghlKSye83tmX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x, y = 2, 5\n",
        "x, y = np.array((x,)), np.array((y,))\n",
        "model.compile('sgd', 'mae')\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cmU7a3ot4CPF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's make some changes. If we change the activity_regularizer for example to l1 norm with a factor of 1 we get the following model"
      ]
    },
    {
      "metadata": {
        "id": "0q4DsP1513rm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = Input([1])\n",
        "output = Dense(1, kernel_initializer=constant(2), bias_initializer=constant(1),\n",
        "               kernel_regularizer=None, bias_regularizer=None, activity_regularizer=l1(1))(input)\n",
        "model = Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi0kDwLi5NyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Based on the summary nothing really changed. and if we predict on the same x the result will be once again 5"
      ]
    },
    {
      "metadata": {
        "id": "hov6wKB0QNBv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = 2\n",
        "y_pred = model.predict(np.array((x,)))\n",
        "y_pred[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_FCw6NE5VUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, the loss this time is different. this happens because the new loss is:\n",
        "$$new\\_loss=loss+regularization$$\n",
        "where in our case the regularization is:\n",
        "$$l_1(a)=\\sum{w\\cdot|a|}$$\n",
        "\n",
        "where $w$ is the argument we define in the $l1()$ function\n",
        "\n",
        "Similarly, we have:\n",
        "$$l_2(a)=\\sum{w\\cdot a^2}$$"
      ]
    },
    {
      "metadata": {
        "id": "PuTOZHgOQyKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x, y = 2, 5\n",
        "x, y = np.array((x,)), np.array((y,))\n",
        "model.compile('sgd', 'mae')\n",
        "loss = model.evaluate(x, y, verbose=0)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frIKU5cQ75zj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fell free to change:\n",
        "- the initial values of the weight and the bias of the model\n",
        "- the type of regularization function for weight, bias or activation\n",
        "- the factor of each regularization function\n",
        "\n",
        "two obtain different results"
      ]
    }
  ]
}